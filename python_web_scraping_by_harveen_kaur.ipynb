{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do people think of Titanic (1997)?\n",
    "## Description: \n",
    "We will divide this case study into 3 parts. Only the first one is required, the other 2 are just bonuses. All code should be written in Python and uploaded to a git repo.\n",
    "\n",
    "## Task 1: \n",
    "Scrape all reviews of Titanic from the following page https://www.imdb.com/title/tt0120338/reviews?ref_=tt_ov_rt (hint: python requests or selenium)\n",
    "\n",
    "\n",
    "## Task 2: \n",
    "Classify reviews into three clusters (hint: unsupervised learning, classification)\n",
    "\n",
    "\n",
    "## Task 3: \n",
    "Extract the 10 most relevant words for each cluster and print them.\n",
    "This will show how good (or how humanly understandable) the classification of the previous task was.\n",
    "You will need to choose a measure for quantifying how \"relevant\" is a word (hint: TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open in headless mode so as to remove the emulator from view\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "# The chromedriver executable is present in the same folder as the notebook\n",
    "# Change the driver path to the correct one if the notebook is to be executed is a different system.\n",
    "driver = webdriver.Chrome(executable_path = './chromedriver', chrome_options=chrome_options)\n",
    "\n",
    "# Movie link given in the question.\n",
    "driver.get(\"https://www.imdb.com/title/tt0120338/reviews?ref_=tt_ov_rt\")\n",
    "\n",
    "# Check if the \"Show More\" button exists. If it exists, we press it so as to obtain the next page of reviews.\n",
    "# We continue till we get no more pages\n",
    "while driver.find_element_by_css_selector(\"#load-more-trigger\"):\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"load-more-trigger\"))\n",
    "        )\n",
    "        element.click()\n",
    "    except:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source of the current website is parsed using beautifulsoup. This source contains all the reviews.\n",
    "html = driver.page_source\n",
    "driver.close()\n",
    "html = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the individual review container through class name selector.\n",
    "user_review_container = html.find_all('div', attrs={'class':'collapsable'})\n",
    "user_review_container_spoilers = html.find_all('div', attrs={'class':'with-spoiler'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format the output from beautifulsoup filters.\n",
    "def format_into_json(review_container):\n",
    "    '''\n",
    "    function to format the scrapted reviews in JSON format.\n",
    "    '''\n",
    "    res_list = []\n",
    "    for review in  review_container:\n",
    "        review_url = review.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "        username = review.find('div', attrs={'class':'display-name-date'}).find('span').find('a').text\n",
    "        user_url = review.find('div', attrs={'class':'display-name-date'}).find('span').find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "        title = review.find('a', attrs={'class':'title'}).text\n",
    "        content = review.find('div', attrs={'class':'content'}).find('div').text\n",
    "        res_list.append({\"review_url\":'https://www.imdb.com'+review_url, \n",
    "                         \"username\":username, \n",
    "                         \"user_url\":'https://www.imdb.com'+user_url, \n",
    "                         \"title\": title, \n",
    "                         \"content\": content, \n",
    "                        })\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_url': 'https://www.imdb.com/review/rw3166784/?ref_=tt_urv',\n",
       " 'username': 'katherinegranada995',\n",
       " 'user_url': 'https://www.imdb.com/user/ur57176161/?ref_=tt_urv',\n",
       " 'title': ' Amazing in 1997, 2005, 2015, 2030, 3010 & forever more a Masterpiece!\\n',\n",
       " 'content': 'You can watch this movie in 1997, you can watch it again in 2004 or 2009 or you can watch it in 2015 or 2020, and this movie will get you EVERY TIME. Titanic has made itself FOREVER a timeless classic! I just saw it today (2015) and I was crying my eyeballs out JUST like the first time I saw it back in 1998. This is a movie that is SO touching, SO precise in the making of the boat, the acting and the storyline is BRILLIANT! And the preciseness of the ship makes it even more outstanding! Kate Winslet and Leonardo Dicaprio definitely created a timeless classic that can be watched time and time again and will never get old. This movie will always continue to be a beautiful, painful & tragic movie. 10/10 stars for this masterpiece!'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Json data from the scraper content.\n",
    "json_data = format_into_json(user_review_container) + format_into_json(user_review_container_spoilers)\n",
    "json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the intermediate result so that we need not run the scraping again.\n",
    "import json\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data again.\n",
    "# If any changes are to be done to the clustering, we only need \n",
    "# to run the cells from this as the parsing logic remains the\n",
    "# same.\n",
    "df = pd.read_json('./data.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_url</th>\n",
       "      <th>username</th>\n",
       "      <th>user_url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.imdb.com/review/rw3166784/?ref_=tt...</td>\n",
       "      <td>katherinegranada995</td>\n",
       "      <td>https://www.imdb.com/user/ur57176161/?ref_=tt_urv</td>\n",
       "      <td>Amazing in 1997, 2005, 2015, 2030, 3010 &amp; for...</td>\n",
       "      <td>You can watch this movie in 1997, you can watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.imdb.com/review/rw5903837/?ref_=tt...</td>\n",
       "      <td>sander-vanluit</td>\n",
       "      <td>https://www.imdb.com/user/ur23952614/?ref_=tt_urv</td>\n",
       "      <td>Why only a 7.8?\\n</td>\n",
       "      <td>There is no movie which made a bigger emotiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.imdb.com/review/rw5954378/?ref_=tt...</td>\n",
       "      <td>sucoaramada</td>\n",
       "      <td>https://www.imdb.com/user/ur122456972/?ref_=tt...</td>\n",
       "      <td>Why low score?\\n</td>\n",
       "      <td>People are crazy. They rate Avengers so high a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.imdb.com/review/rw5503104/?ref_=tt...</td>\n",
       "      <td>MR_Heraclius</td>\n",
       "      <td>https://www.imdb.com/user/ur87850731/?ref_=tt_urv</td>\n",
       "      <td>Great\\n</td>\n",
       "      <td>Very beautiful and cinematic movie with lots o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.imdb.com/review/rw4224702/?ref_=tt...</td>\n",
       "      <td>paulclaassen</td>\n",
       "      <td>https://www.imdb.com/user/ur2263198/?ref_=tt_urv</td>\n",
       "      <td>Despite a lot of plot flaws and conveniences,...</td>\n",
       "      <td>Ah, yes, the film that propelled Leonardi DiCa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          review_url             username  \\\n",
       "0  https://www.imdb.com/review/rw3166784/?ref_=tt...  katherinegranada995   \n",
       "1  https://www.imdb.com/review/rw5903837/?ref_=tt...       sander-vanluit   \n",
       "2  https://www.imdb.com/review/rw5954378/?ref_=tt...          sucoaramada   \n",
       "3  https://www.imdb.com/review/rw5503104/?ref_=tt...         MR_Heraclius   \n",
       "4  https://www.imdb.com/review/rw4224702/?ref_=tt...         paulclaassen   \n",
       "\n",
       "                                            user_url  \\\n",
       "0  https://www.imdb.com/user/ur57176161/?ref_=tt_urv   \n",
       "1  https://www.imdb.com/user/ur23952614/?ref_=tt_urv   \n",
       "2  https://www.imdb.com/user/ur122456972/?ref_=tt...   \n",
       "3  https://www.imdb.com/user/ur87850731/?ref_=tt_urv   \n",
       "4   https://www.imdb.com/user/ur2263198/?ref_=tt_urv   \n",
       "\n",
       "                                               title  \\\n",
       "0   Amazing in 1997, 2005, 2015, 2030, 3010 & for...   \n",
       "1                                  Why only a 7.8?\\n   \n",
       "2                                   Why low score?\\n   \n",
       "3                                            Great\\n   \n",
       "4   Despite a lot of plot flaws and conveniences,...   \n",
       "\n",
       "                                             content  \n",
       "0  You can watch this movie in 1997, you can watc...  \n",
       "1  There is no movie which made a bigger emotiona...  \n",
       "2  People are crazy. They rate Avengers so high a...  \n",
       "3  Very beautiful and cinematic movie with lots o...  \n",
       "4  Ah, yes, the film that propelled Leonardi DiCa...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the data is valid.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sanitize the text.\n",
    "def custom_initial_clean(phrases_X):\n",
    "    phrases_X = phrases_X.str.replace('\\\\*', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace('\\\\/', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace('\\\\\\\\', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace('\\n', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace('\\t', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace('-', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r'/', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r'``', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r'`', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"''\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\",\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"\\.$\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\":\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"# \", '#', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\";\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"?\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"=\", ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(\"...\", ' ', regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"..\", ' ', regex=False)\n",
    "    phrases_X = phrases_X.str.replace('<br>', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace('</br>', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r'LRB', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r'RRB', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[C|c]a n't\", 'cannot', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[W|w]o n't\", 'will not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[W|w]ere n't\", 'were not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[W|w]as n't\", 'was not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[W|w]ould n't\", 'would not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[D|d]oes n't\", 'does not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[I|i]s n't\", 'is not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[C|c]ould n't\", 'could not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[D|d]id n't\", 'did not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[H|h]as n't\", 'has not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[H|h]ave n't\", 'have not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[D|d]o n't\", 'do not', regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[A|a]i n't\", \"not\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[N|n]eed n't\", \"need not\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[A|a]re n't\", \"are not\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[S|s]hould n't\", \"should not\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[H|h]ad n't\", \"had not\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"https?://\\S+\", \"\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"<.*?>\", \"\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(f\"[{re.escape(punctuation)}]\", \"\", regex=True)\n",
    "    phrases_X = phrases_X.str.replace(r\"[^A-Za-z0-9\\s]+\", \"\", regex=True)\n",
    "\n",
    "    phrases_X = phrases_X.str.replace(\" 's\", \" \", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'s\", \"\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'ve\", \"have\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'d\", \"would\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'ll\", \"will\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'m\", \"am\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'n\", \"and\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'re\", \"are\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\"'til\", \"until\", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\" ' \", \" \", regex=False)\n",
    "    phrases_X = phrases_X.str.replace(\" '\", \" \", regex=False)\n",
    "\n",
    "    phrases_X = phrases_X.str.replace(r'[ ]{2,}', ' ', regex=True)\n",
    "    phrases_X = phrases_X.str.lower()\n",
    "    return phrases_X\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Helper function to tokenize the sanitized text.\n",
    "def tokenize_the_text(phrases):\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.text import Text\n",
    "    \n",
    "    tokens = [word for word in phrases]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word_tokenize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Helper function to remove the stop words from the tokens. This is done to remove words\n",
    "# that do not add meaning to the sentance and exist only to satisy the grammatical requirements\n",
    "# of the language. \n",
    "def removing_stopwords(tokens_custom_cleaned):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens_custom_cleaned_and_without_stopwords = []\n",
    "    for sentence in tokens_custom_cleaned:\n",
    "        tokens_custom_cleaned_and_without_stopwords.append([word for word in sentence if word not in stop_words])\n",
    "        \n",
    "    return tokens_custom_cleaned_and_without_stopwords\n",
    "\n",
    "# Helper function to lemmatize the tokens.\n",
    "def lemmatizing_the_tokens(tokens_custom_cleaned_and_without_stopwords):\n",
    "\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer \n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    tokens_custom_cleaned_and_without_stopwords_and_lemmatized = []\n",
    "\n",
    "    for sentence in tokens_custom_cleaned_and_without_stopwords:\n",
    "        tokens_custom_cleaned_and_without_stopwords_and_lemmatized.append([lem.lemmatize(word, pos='v') for word in sentence])\n",
    "        \n",
    "    return tokens_custom_cleaned_and_without_stopwords_and_lemmatized\n",
    "\n",
    "# Helper function to find the vocab list from the corpus.\n",
    "def create_a_vocab(tokens):\n",
    "    \n",
    "    vocab = set()\n",
    "\n",
    "    for setence in tokens:\n",
    "        for word in setence:\n",
    "            vocab.add(word)\n",
    "\n",
    "    vocab = list(vocab)\n",
    "\n",
    "    return vocab\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize the title and content from the dataframe.\n",
    "title, content = custom_initial_clean(df.title.copy()), custom_initial_clean(df.content.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the title and content so as to simplify the tokenizing.\n",
    "data = title + content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tfidf vectorizer from nltk.\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "X = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature map so as to do a reverse lookup.\n",
    "feature_names = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means with k = 3\n",
    "km = KMeans(n_clusters=3, max_iter=200, n_init=10)\n",
    "km = km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fitted model against the data so as to classify the data into clusters.\n",
    "predictions = defaultdict(list)\n",
    "for i in range(X.shape[0]):\n",
    "    x = X[i]\n",
    "    predictions[km.predict(x)[0]].append(title.values[i])\n",
    "    predictions[km.predict(x)[0]].append(content.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epic' 'romantic' 'omg' 'meh' 'negative' 'best' 'blah' 'naaaah' 'awful'\n",
      " 'favourite']\n",
      "['amazing' 'classic' 'zarina' 'finally' 'cool' 'awesome' 'magic' 'amazing'\n",
      " 'epochal' 'titanic']\n",
      "['overrated' 'good' 'meh' 'titanic' 'masterpiece' 'titanic' 'romantic'\n",
      " 'classic' 'haunting' 'complimentary']\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get the most prominent words from the input\n",
    "def get_top_tf_idf_words(response, top_n=10):\n",
    "    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
    "    return feature_names[response.indices[sorted_nzs]]\n",
    "\n",
    "# Get the top 10 most significant words from the 3 clusters.\n",
    "for i in range(3):\n",
    "    print(get_top_tf_idf_words(vectorizer.transform(predictions[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
